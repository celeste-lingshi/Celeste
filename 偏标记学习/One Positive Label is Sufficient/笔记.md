# One Positive Label is Sufficient: Single-Positive Multi-Label Learning with Label Enhancement

本文提出了一种新的SPMLL方法SILE，即带标签增强的单正多标签学习方法。具体地说，给出了一个无偏风险估计器，它可以保证在全监督学习中近似收敛到最优风险最小化器，并表明每个实例的一个正标签足以训练一个模型。然后，通过恢复潜在软标签作为标签增强过程，建立相应的经验风险估计器，其中潜在软标签的后验密度近似于由推理模型参数化的变分Beta密度。


* 多类分类（Multiclass Classification）
一个样本属于且只属于多个类中的一个，一个样本只能属于一个类，不同类之间是互斥的。
典型方法：
  * One-vs-All or One-vs.-rest：
将多类问题分成N个二类分类问题，训练N个二类分类器，对第i个类来说，所有属于第i个类的样本为正（positive）样本，其他样本为负（negative）样本，每个二类分类器将属于i类的样本从其他类中分离出来。

   * one-vs-one or All-vs-All：
训练出N(N-1)个二类分类器，每个分类器区分一对类(i,j)。

* 多标签分类(multilabel classification)
又称，多标签学习、多标记学习，不同于多类分类，一个样本可以属于多个类别（或标签），不同类之间是有关联的。

***

在本文中，我们提出了一种理论上有保证的方法，称为 SMILE，即带有标签增强的单正多标签学习。具体来说，首先推导出一个无偏风险估计器，这表明每个实例的一个正标签足以训练多标签学习的预测模型。此外，推导了估计误差界限，保证了所提出方法的风险一致性。然后，我们可以通过在标签增强过程中恢复与每个示例对应的软标签来设计基准解决方案 ，其中潜在软标签的后验密度是通过利用近似的 Beta 密度来推断的。贡献总结如下：

