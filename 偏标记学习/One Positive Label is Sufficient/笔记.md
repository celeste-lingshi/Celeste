# One Positive Label is Sufficient: Single-Positive Multi-Label Learning with Label Enhancement

本文提出了一种新的SPMLL方法SILE，即带标签增强的单正多标签学习方法。具体地说，给出了一个无偏风险估计器，它可以保证在全监督学习中近似收敛到最优风险最小化器，并表明每个实例的一个正标签足以训练一个模型。然后，通过恢复潜在软标签作为标签增强过程，建立相应的经验风险估计器，其中潜在软标签的后验密度近似于由推理模型参数化的变分Beta密度。


* 多类分类（Multiclass Classification）
一个样本属于且只属于多个类中的一个，一个样本只能属于一个类，不同类之间是互斥的。
典型方法：
  * One-vs-All or One-vs.-rest：
将多类问题分成N个二类分类问题，训练N个二类分类器，对第i个类来说，所有属于第i个类的样本为正（positive）样本，其他样本为负（negative）样本，每个二类分类器将属于i类的样本从其他类中分离出来。

  * One-vs-One or All-vs-All：
训练出N(N-1)个二类分类器，每个分类器区分一对类 $(i,j)$ 。

* 多标签分类(multilabel classification)
又称，多标签学习、多标记学习，不同于多类分类，一个样本可以属于多个类别（或标签），不同类之间是有关联的。

***

在本文中，我们提出了一种理论上有保证的方法，称为 SMILE，即带有标签增强的单正多标签学习。具体来说，首先推导出一个无偏风险估计器，这表明每个实例的一个正标签足以训练多标签学习的预测模型。此外，推导了估计误差界限，保证了所提出方法的风险一致性。然后，我们可以通过在标签增强过程中恢复与每个示例对应的软标签来设计基准解决方案 ，其中潜在软标签的后验密度是通过利用近似的 Beta 密度来推断的。贡献总结如下：

1. 从理论上，我们首次推导出了SPMLL的无偏风险估计。在此基础上，建立了保证风险一致性的估计误差界，并证明了在全监督MLL中得到的风险最小值将近似收敛于最优风险最小值。
2. 在实践中，我们提出了利用标签增强恢复潜在软标签的SMILE方法。利用近似贝塔密度推导出潜在软标签的后验密度，并推导出优化的证据下限(ELBO)。


# 相关工作

在多标签学习中，每个示例同时与多个类标签相关联。由于 MLL 中的输出空间与类标签的数量呈指数关系，因此提出了许多方法来利用标签相关性来促进学习过程。一阶方法将 MLL 问题分解为许多二元分类问题。二阶方法考虑标签对之间的标签相关性。高阶方法进一步关注标签集之间的标签相关性。另一项研究侧重于通过将特定于标签的特征形式化到每个类标签，从而操纵特征空间。

此外，一些工作侧重于通过深度模型处理MLL。通过使用GCN在所有标签节点之间传播信息。还有使用Transformer通过引入三进制编码方案来表示标签的状态，用于探索标签依赖性


# 问题设置

## 多标记学习
在MLL中，每个示例都与多个标签相关联，旨在构建一个预测模型，该模型可以为不可见的实例分配一组相关标签。设 $\mathcal X = \mathbb R^q$ 是q维实例空间，$\mathcal Y= \{ 1，2，\cdots，c \}$ 是带有 $c$ 类标签的标签空间。给定MLL训练集 $\mathcal D = \{(x_i,Y_i)| 1 \leq i \leq n\}$ 其中 $x_i∈ \mathcal X$ 表示 $q$ 维实例， $Y_i \in \mathcal C$ 是与 $x_i$ 相关的一组相关标签，其中 $\mathcal C=2^{\mathcal Y}$ 。多标签学习的任务是诱导一个多标签分类器 $f:\mathcal X \to 2^{\mathcal Y}$ ，将以下分类风险降至最低：
$$R(f) = \mathbb E_{p(x,Y)} [\mathcal L(f(x),Y)]$$

其中 $\mathcal L : \mathbb R^q \times 2^{\mathcal Y} \to \mathbb R_+$

## 单正多标记学习

SPMLL训练集 $\tilde {\mathcal D} = \{(x_i,\gamma_i) | 1 \leq i \leq n  \}$ 其中 $\gamma_i \in \mathcal Y$ 表示实例 $x_i$ 被观测到的单正标签。 $\gamma_i \in Y_i$ ，但相关标签集 $Y_i$ 并不可以直接被学习算法访问。

对于每个SPMLL训练示例 $(x_i， \gamma_i)$ ，我们使用观察到的单正向量 $l_i = [l^1_i, l^2_i,\cdots,l^c_i]^T \in \{ 0,1\}^c$ 来表示第 $j$ 个标记是否是观测到的正标记，即如果 $j = \gamma_i$ ，则 $l^j_i = 1$ ，否则 $l^j_i = 0$ 。多标签向量用 $y_i = [y^1_i, y^2_i ,\cdots ,y^c_i]^T \in \{0,1\}^c$ 表示。其中，如果第 $j$ 个标签与 $x_i$ 相关，则 $y^j_i = 1$ ，否则 $y^j_i = 0$ 。SPMLL的任务是从 $\tilde{D}$ 中推导出一个多标签分类器 $f: \mathcal X \to 2^{\mathcal Y}$，该分类器可以为未见实例分配一组相关的标签集。

凭经验验证了 SPMLL 将减少监督量，并对分类性能造成可容忍的损害。直观的解AN是假设未被观察到的标签都是负标签，这导致了引入一些假负标签的缺点。因此，SOTA方法旨在通过利用DNN的学习能力来减少假负标签的破坏性影响，在实践中取得了良好的性能。

然而，目前还没有可以提供理论见解的方法。


# 方法

## 风险一致性估计量

为处理单正多标签学习，分类风险 $R(f)$ 可改写为

![](./图片/%E9%A3%8E%E9%99%A9%E4%B8%80%E8%87%B4%E4%BC%B0%E8%AE%A1%E5%99%A8.png)

使用二值交叉熵函数来定义损失，有

$$\begin{align}
\mathcal L(f(x),Y) &= \sum_{j \in Y} \log f_j (x) + \sum_{j \notin Y} \log(1-f_j(x)) \\
&= \sum_{j \in Y} \ell^j + \sum_{j \notin Y} \bar \ell^j
\end{align}$$

其中 $\ell^j=\log f_j(x)$ ， $\bar \ell^j = 1-f_j(x)$ 

$$ \sum_{Y \in C} \mathcal L(f(x),Y)p(Y|x) = \sum^c_{j=1} d^j\ell^j+(1-d^j)\bar\ell^j$$

$d^j = p(y^j=1|x) \in [0,1]$ 可以视为样本 $x$ 对应的对第 $j$ 类的软标签，因此 $R_{sp}(f)$ 可以写成：

$$R_{sp}(f) = \mathbb E_{p(x,\gamma)} \left[ \frac1{p(y^\gamma = 1|x)c} \sum_{j=1}^c d^j\ell^j +(1-d^j)\bar\ell^j \right]$$

经验风险估计器可以表示为：

$$\hat R_{sp}(f) = \frac1 n \sum^n_{i=1} \left( \frac1 {p(y^{\gamma_i} = 1|x_i)c} \sum^c_{j=1} d^j_i\ell^j_i + (1-d^j_i)\bar\ell^j_i\right)$$

然后，我们可以通过在 $f_{\gamma_i}(x_i)$上应用 $sigmoid$ 函数来近似 $p(y^{\gamma_i}=1|x_i)$ ，并通过后面的标签增强过程恢复每个示例对应的软标签 $d^j_i$ 来设计基准解决方案。

## 标签增强

为了恢复软标签向量 $d_i = [d^1_i,d^2_i,\cdots, d^c_i] ^T \in [0,1]^c$ SMILE 考虑特征空间的拓扑信息，并估计邻接矩阵 $A=[a_{ij}]_{n\times n}$ ：
$$a_{ij} = 
  \begin{cases}
  1 & if \quad  x_i \in \mathcal N(x_j)\\
  0 & otherwise \\
  \end{cases}
$$

其中 $\mathcal N(x_j)$ 表示 $x_j$ 的 $k$ 邻近集

我们假设潜在软标签矩阵 $D=[d_1,d_2,\cdots,d_n]$ 生成观察到的逻辑标签矩阵 $L = [l_1,l_2,\cdots, l_n]$ 和邻接矩阵 $A$ 。此外观察到的实例矩阵 $X = [x_1,x_2,\cdots, x_n]$ 是由 $D$ 和潜在特征矩阵 $Z = [z_1,z_2,\cdots,z_n]$ 生成。我们假设先验密度 $p(d)$ 是一个Beta密度，其次值为：$\hat \alpha = [\hat \alpha^1, \hat \alpha^2,\cdots, \hat \alpha^c]$  和 $\hat \beta = [\hat \beta^1, \hat \beta^2,\cdots, \hat \beta^c]$。即 $p(d) = \prod^c_{j=1} Beta(d^j | \hat \alpha^j , \hat \beta^j)$ 。然后先验密度 $p(D)$ 可以是每个 $p(d)$ 的乘积。此外，我们假设先验密度 $p(z)$ 是一个标准的高斯函数，先验密度 $p(Z)$ 可以表示为每个高斯 $p(Z)= \prod^n_{i=1}{Gau(z_i|0,1)}$ 。后验密度 $p(D,Z|L,X,A)$ 可以分解如下：

$$p(D,Z|L,X,A) = p(D|L,X,A)p(Z|D,L,X,A) = p(D|L,X,A)p(Z|D,X)$$


***
## ***关于Beta密度***

伯努利试验（同样的条件下重复地、相互独立地进行的一种随机试验，其特点是该随机试验只有两种可能结果：发生或者不发生）频率学派的观点（出现次数最多的情况体现了概率的分布），体现了 *后验*

**Gamma函数**：阶乘在实数域的推广。

$$\Gamma(x) = \int_0^\infty t^{x-1} e^{-t} dt\\
\Gamma(n) = (n-1)!$$

Beta分布一般被用于建模伯努利试验事件成功的概率的概率分布。

**Beta分布的概率密度函数(PDF)**:
$$f(x) = \frac 1 {Beta(\alpha,\beta)} x^{\alpha-1} (1-x)^{\beta-1} , x\in[0,1] \\
Beta(\alpha,\beta) = \int_0^1 x^{\alpha-1} (1-x)^{\beta-1}dx = \frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha+\beta)}$$


***


