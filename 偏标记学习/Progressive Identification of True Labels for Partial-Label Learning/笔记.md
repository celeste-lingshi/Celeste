# Progressive Identification of True Labels for Partial-Label Learning

本文的目标是提出一种新的PLL框架，该框架具有灵活的模型和优化算法。更具体地说，提出了一种新的分类风险估计器，并从理论上分析了分类一致性，建立了估计误差界。然后我们提出了一种渐进式识别算法，用于近似最小化所提出的风险估计量,其中模型的更新和真实标签的识别是无缝的。该算法具有模型无关和损失无关的特点，与随机优化算法兼容。

***

    KL散度：是一个用来衡量两个概率分布的相似性的一个度量指标。

1、 **熵：**    
信息论中，某个信息$ x_i $出现的不确定性大小成为其所携带的信息量，用$ I(x_i) $表示。  
称$H(x) = - \sum ^n_{i=1} P(x_i) log P(x_i)$ 为信息熵，连续信息的信息熵为$ H(x) = - \int f(x) log f(x)dx $

*交叉熵：*$H(P,Q) = - \sum ^n_{i=1} P(x_i) log Q(x_i)$

***
2、**KL散度：**
KL散度又可称为相对熵，描述两个概率分布P和Q的差异或者相似性，用$D_{KL} (P || Q)$ 表示，

$$ \begin{align}
D_{KL}(P||Q) & = H(P,Q) - H(P)\\  
&= \sum_i P(x_i) log \frac 1 {Q(x_i)} - \sum_i P(x_i) log \frac 1 {P(x_i)} \\
&= \sum_i P(x_i) log \frac {P(x_i)} {Q(x_i)}
 \end{align}$$

散度越小，说明概率Q与概率P之间越接近，那么估计的概率分布与真实的概率分布也就越接近。
KL散度的性质：
*  非对称性： $ D_{KL} (P||Q) \ne D_{KL} (Q||P)$
*  $ D_{KL} (P||Q) \ge 0 ,仅在 P=Q 时等号成立$



***
3、**凸函数:**  设函数 $f(x)$ 在区间 $I$ 上连续，如果对 $I$ 上任意两点 $x_{1},x_{2}$ 恒有
$$ tf(x_1) + (1-t)f(x_2) \ge f(tx_1+(1-t)x_2)$$

则称函数 $f(x)$ 在 $I$ 上是（向下）凸的。这里的 $t$ 是个参数，$0\leqslant t\leqslant 1$。
令  $t=\frac{1}{2}$ 有：
$$\frac{f(x_{1})+f(x_{2})}{2}\geq f(\frac{x_{1}+x_{2}}{2})$$
***
4、**Jensen不等式：** Jensen 不等式与凸函数是密切相关的。可以说 Jensen 不等式是凸函数的推广，而凸函数是 Jensen 不等式的特例。
对于任意点集$\left \{ x_{i} \right \}$，凸函数 $f(x)$ 满足

$$\sum_{i}^{}\lambda _{i}f(x_{i})\geqslant f(\sum_{i}^{}\lambda _{i}x_{i})$$
这就是 Jensen 不等式。

***


## PLL发展背景


PLL风险估计量定义在$p(x, s)$上:
$$\mathcal R_{PLL}(g) = \mathbb E_{(X,S)\sim p(x,s)} [l_{PLL}(g(X),S)] $$

之前的方法求解效率低下，与高效随机优化不相容。因此，他们几乎无法处理大规模数据集。而最新的工作使用带有随机优化器的 DNN 作为其算法的主干，但是，他们将网络限制在某些特定的架构上，而我们的方法在学习模型上是灵活的，而且他们也缺乏对其方法的理论理解。


## 分类器一致的风险估计器

直观的方法是通过同等对待所有候选标签的代理损失 
$$ l_{PLL}(g(X),S) = \frac 1 {|S|}\sum_{i \in S} l(g(X),e^i) $$
尽管如此，真实标签可能会被多个误报标签的分散注意力的输出所淹没。因此，我们认为只有真正的标签有助于检索分类器。抓住这个想法，我们将 PLL 损失定义为候选标签集的最小损失：

$$ l_{PLL}(g(X),S) =\min_{i \in S} l(g(X),e^i) $$

$$\mathcal R_{PLL}(g) = \mathbb E_{(X,S)\sim p(x,s)} \min_{i \in S}l(g(X),e^i) $$

令$Y^g = \argmin_{i \in S} l(g(X),e^i)$
作为分类器g对真实标签的最佳猜测

因此
$$ l_{PLL}(g(X),S) =\min_{i \in S} l(g(X),e^i) = l(g(X),e^{Y^g}) $$


* 引理1：模糊度定义为：
$\gamma = \sup_{(X,Y)\sim p(x,y),\bar Y \in \mathcal Y, S\sim p(s|x,y),\bar Y \ne Y} P_r(\bar Y \in S)$  当 $\gamma < 1$ ，即在小模糊度条件下，PLL问题为ERM可学习性。

    $ \gamma $ 是负标签$\bar Y$与真标签$Y$同时出现的最大概率。模糊度小意味着除了真实的标签，没有其他标签会百分百包含在候选标签集中，这保证了在任何实例上产生的分类错误都会以至少$1−\gamma$的概率被检测到。

然后给出了普通最优分类器可识别的一个条件。
* 引理2：如果$\mathcal l$ 是交叉熵损失或者均方误差损失，最优分类器$g^*满足 g^*_i(X)= p(Y=i|X)$


